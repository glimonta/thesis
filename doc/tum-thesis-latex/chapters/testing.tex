\chapter{Testing}\label{chapter:testing}

We have already formalized the semantics for Chloe and described the translation process to C code, now we proceed to describe the testing process done to increase the trust in our translation process.
We guarantee that the code generated from our semantics will either end with an out of memory error or it will yield the same result as the same program executed in our semantics.
Now we will define what \textit{same} in this context means.
We say that the result of an execution of the semantics and the execution of the generated program in the machine is the same if the final state yielded by both are the same.
This means that we compare the final state yielded by our semantics and verify that after executing the generated program the state is the same i.e.\ the contents of the reachable memory and the global variables is the same.
When generating programs we can either just generate the C code by itself or we can include a set of extra tests in the form of C macros that will guarantee what we mentioned before.

In the following sections we will proceed to describe the test harness created in the Isabelle environment and the one created in the form of C macros in order to generate tests for our code.
We will also describe the meaning of two final states being the same more in detail.
Finally, we will talk about a set of tests and example programs written in the semantics.
We only generate code for valid programs, this means that we will go to an erroneous state if any undefined behavior arises.
In this set of tests we include programs which we expect to go to an erroneous state because they present undefined behavior or border cases, C code will not be generated for these programs.
We also present some example programs such as sorting algorithms to demonstrate how our semantics and code generation process work.


\section{Equality of final states}

We consider a final state yielded by the execution of our semantics equal to a final state of its generated C program, when, at the end of the execution, the values of the global variables are the same for both cases and every block of reachable memory has the same content.
We will now proceed to detail how we check for this equality between final states by the use of tests.

\subsection{Generation of Tests}

We can generate tests for our programs, these will be executed at the end of the execution of the program and will test that the final state of the generated program is the same as the final state from the execution of the semantics.
We compare these final states by checking the values of global variables at the end of an execution against the ones in the final state of the execution of the semantics.

The direction in which the testing is done is by taking the values from the final state of the semantics' execution and checking whether the execution of the generated code has the same values we expect it to have according to the execution of the semantics.

We will now introduce which kind of tests are done depending on whether the content of a global variable is an integer value or a pointer value.

\subsubsection{Integer Values}

When the content of a global variable we want to check is an integer value the test check we generate for it is very simple.
We must simply generate a test that will check whether the integer value in the global variable at the end of the execution of the C code is the same value as the one we get from the global variables valuation in the final state of the execution of the semantics.

\subsubsection{Pointers}

With the checks for pointers we have two cases.
We have the null pointer and the non-null pointer case.

For null values we will generate a test, similar to the one for integer values, where we check if the content of the global variable is \verb|NULL|.

In the case of pointer values different from null this check gets a bit more complicated.
We will have a pointer to a block in the memory and we want to check if the content of that block in memory at the end of the generated program's execution is the same as the content of that same block in our final state in the semantics.
That complete block qualifies as reachable memory which is why we must check the content of each cell in the block.
For each cell in the block we will generate checks depending on what the expected content in the memory cell is, i.e\ an integer value, a null value or a pointer.

In the case of the pointer value checks we will follow every pointer until we reach an integer value, in which case we will generate an integer kind of test or we reach a pointer we already followed in which case we know that the path does not contain any pointer to invalid memory.
We check that the address for the beginning of the block is the same as the one we get from adjusting the pointer we followed to the beginning of the block.
In order to do this we must follow pointers in a certain order and maintain a set of already \textit{discovered} blocks of memory, this way when we find a pointer to a block of memory we already checked (or \textit{discovered}) we can stop and compare the pointer values instead of following the pointers in a cyclic way indefinitely.

We present here the intuitive idea behind our tests generation and will describe the implementation details for the test harnesses, both in Isabelle and in C, in the following sections.

\section{Test Harness in Isabelle}

In this section we will introduce the Isabelle test harness that assists us in the generation of tests for our programs.
First, we define a new datatype for every kind of test instruction we can generate.
We can see this definition in figure~\ref{fig:test_harness_datatype}.


\begin{figure}
\begin{lstlisting}[mathescape=true]
datatype test_instr =
  Discover string nat
| Assert_Eq string int_val
| Assert_Eq_Null string
| Assert_Eq_Ptr string nat

fun adjust_addr :: "int $\Rightarrow$ string $\Rightarrow$ string"
  where
  "adjust_addr ofs ca = shows_binop (shows ca) (''-'') (shows ofs) ''''"

definition ofs_addr :: "int $\Rightarrow$ string $\Rightarrow$ string"
  where
  "ofs_addr ofs ca =
    (shows ''*'' o
      shows_paren (shows_binop (shows ca) (''+'') (shows ofs))) ''''"

definition base_var_name :: "nat $\Rightarrow$ string" where
  "base_var_name i $\equiv$ ''__test_harness_x_'' @ show i"
\end{lstlisting}

\caption{Definitions for the test harness}
\label{fig:test_harness_datatype}
\end{figure}

We have four different test instructions we can generate:

\begin{itemize}
  \item{\verb|Discover| represents an instruction that adds a block to the list of our \textit{discovered} blocks.
  The \verb|string| stands for the string representation in C of the expression and the \verb|nat| stands for the identification number of the current memory block.
  The actual addresses for the allocated memory blocks will vary with every execution of the program in the machine.
  The discover instruction pairs the actual address of a beginning of a block with the base block number to which it corresponds in our abstract representation.
  For this purpose we generate local variables with the function \verb|base_var_name| that are called \verb|__test_harness_x_|$n$ where $n$ represents the identification number for the block and in those variables we will save the actual address for the beginning of the block for that particular execution.}
  \item{\verb|Assert_Eq| represents an instruction that will check that the value to which an expression evaluates is the same as the integer value we expect it to have.
  The \verb|string| stands for the string representation in C of the expression and the \verb|int_val| stands for the value we expect that variable to have according to our final state in the semantics execution.}
  \item{\verb|Assert_Eq_Null| represents an instruction that will check that the value to which an expression evaluates is the null pointer.
  The \verb|string| stands for the string representation in C of the expression.}
  \item{\verb|Assert_Eq_Pointer| represents an instruction that will check that the pointer value to which an expression evaluates points to the same block we expect it to point.
  where \verb|string| stands for the string representation in C of the expression and the \verb|nat| stands for the identification number for the block of memory.}
\end{itemize}

We also have some auxiliary functions that aid us in the test generation process.
The function \verb|adjust_addr| will take an offset and a string representation of a c expression (which evaluates to a pointer) and yield a string representation that adjusts the address to the beginning of the block by subtracting the offset from it.
The function \verb|ofs_addr| will take an offset and a string representation of a c expression (which evaluates to a pointer) and yield a string representation that adjusts the address to point to the specific cell in the specified offset by adding it to the address.
Finally, the function \verb|base_var_name| given a natural number $n$ yields a variable we use for testing which will be used to save the address to the beginning of block $n$.
This created variable will always have the prefix \verb|__test_harness_x_| plus the number $n$.
For example, for block number $2$ the function will yield the string ``\verb|__test_harness_x_2|''.


\begin{figure}
\begin{lstlisting}[mathescape=true]
context fixes $\mu$ :: mem begin

partial_function (option) dfs
  :: "nat set $\Rightarrow$ addr $\Rightarrow$ string $\Rightarrow$ (nat set $\times$ test_instr list) option"
  where
  [code]: "dfs D a ca = do {
    let (base,ofs) = a;

    case $\mu$!base of
      None $\Rightarrow$ Some (D,[])
    | Some b $\Rightarrow$ do {
        let ca = adjust_addr ofs ca;
        if base $\notin$ D then do {
          let D = insert base D;
          let emit = [Discover ca base];

          fold_option ($\lambda$i (D,emit). do {
            let i=int i;
            let cval = (ofs_addr i (base_var_name base));
            case b!!i of
              None $\Rightarrow$ Some (D,emit)
            | Some (I v) $\Rightarrow$ Some (D,emit @ [Assert_Eq cval v])
            | Some (NullVal) $\Rightarrow$ Some (D,emit @ [Assert_Eq_Null cval] )
            | Some (A addr) $\Rightarrow$ do {
                (D,emit') $\leftarrow$ dfs D addr cval;
                Some (D,emit@emit')
              }
          })
            [0..<length b]
            (D,emit)

        } else do {
          Some (D,[Assert_Eq_Ptr ca base])
        }
      }
  }"
end
\end{lstlisting}

\caption{DFS for test generation}
\label{fig:dfs_test}
\end{figure}

Previously we talked about following pointers until we reach either an integer or null value or until we reach a pointer which we already followed.
In order to do this we must follow pointers in a certain order and maintain a set of already \textit{discovered} blocks of memory, this way when we find a pointer to a block of memory we already checked (or \textit{discovered}) we can stop and compare the pointer values instead of following the pointers in a cyclic way indefinitely re-checking parts of the memory which we already checked.
We follow the pointers in depth-first search (DFS).
In figure~\ref{fig:dfs_test} we have the algorithm used for following a pointer.
It takes a set of natural numbers which are the blocks we have already discovered, an address (the one we are following), a string which is the string representation of the expression in C which should contain this same address and will yield a new set of discovered blocks and a list of test instructions we have to generate.

The algorithm goes as follows:
First, we try to index the block, to see whether the memory is free or holds some content.
If the memory is free we will return the same discovered set and will generate no extra instructions.
Next, we will adjust the address to the start of the block, this is because when checking the memory we want to check the complete blocks since they are a part of the reachable memory.
If the block we are currently checking is already in the discovered set we will simply return the same discovered set and a \verb|Assert_Eq_Ptr| instructions to check the pointers.
If, on the other hand, the block we are currently checking is a new block we have not \textit{seen} we proceed to insert it to the discovered set and add a \verb|Discover| instruction to the list of instructions generated.

We will then proceed to check the contents of the block of memory, starting from the first cell up until the last cell of that block.
When checking each cell we will check whether the content of the cell is an integer value, a null value or an address.
If the cell contains an integer value, we return the same discovered set and we add a \verb|Assert_Eq| instruction to the list of test instructions to generate.
If the cell contains a null value, we return the same discovered set and we add a \verb|Assert_Eq_Null| instruction to the list of test instructions to generate.
Finally, if the cell contains an address, we will proceed to follow that address and do a recursive call to \verb|dfs| before continuing to check the current memory block and after this call returns we return the new discovered set from the recursive call and we append the list of test instructions we had generated so far with the list of instructions that the recursive call returns.


\section{Test Harness in C}

In order to support the testing in C we need to have some macros that will correspond to the ones generated in Isabelle.
We write a C header file where we define the macros necessary to do the tests and also some useful variables.

This header file can be seen in figure~\ref{fig:header_test_harness}.
There we can find the definitions for the macros that correspond to the \verb|Discover|, \verb|Assert_Eq|, \verb|Assert_Eq_Null| and \verb|Assert_Eq_Ptr| instructions.
For maintaining the discovered set we use a hash set.
The implementation of the hash set is done by Sergey Avseyev and it is available online~\parencite{hashset}.
We also define three variables that help us keep track of the number of tests made as well as how many passed and how many failed and a variable that contains the discovered hash set.


\begin{figure}
\begin{lstlisting}[mathescape=true]
#include <stdlib.h>
#include <stdio.h>
#include <limits.h>
#include <inttypes.h>
#include "hashset.h"

hashset_t __test_harness_discovered;
int __test_harness_num_tests = 0;
int __test_harness_passed = 0;
int __test_harness_failed = 0;

#define __TEST_HARNESS_DISCOVER(addr, var)
  hashset_add(__test_harness_discovered, addr); var = addr;

#define __TEST_HARNESS_ASSERT_EQ(var, val)
  ++__test_harness_num_tests;
  (var != val) ? ++__test_harness_failed : ++__test_harness_passed;

#define __TEST_HARNESS_ASSERT_EQ_NULL(var)
  ++__test_harness_num_tests;
  (var != NULL) ? ++__test_harness_failed : ++__test_harness_passed;

#define __TEST_HARNESS_ASSERT_EQ_PTR(var, val)
  ++__test_harness_num_tests;
  (var != val) ? ++__test_harness_failed : ++__test_harness_passed;
\end{lstlisting}

\caption{Header file test\_harness.h}
\label{fig:header_test_harness}
\end{figure}


The test instructions are pretty printed to C macros by using the \verb|test_instructions| function:

\begin{lstlisting}[mathescape=true, frame=single]
definition tests_instructions :: "test_instr list $\Rightarrow$ nat $\Rightarrow$ shows" where
  "tests_instructions l ind $\equiv$ foldr ($\lambda$
      (Discover ca i) $\Rightarrow$
        indent_basic ind
          (shows ''__TEST_HARNESS_DISCOVER '' o
            shows_paren
             ( shows ca o shows '', '' o shows (base_var_name i)))
    | (Assert_Eq ca v) $\Rightarrow$
        indent_basic ind
          (shows ''__TEST_HARNESS_ASSERT_EQ '' o
            shows_paren ( shows ca o shows '', '' o shows v))
    | (Assert_Eq_Null ca) $\Rightarrow$
        indent_basic ind
          (shows ''__TEST_HARNESS_ASSERT_EQ_NULL '' o
            shows_paren ( shows ca ))
    | (Assert_Eq_Ptr ca i) $\Rightarrow$
        indent_basic ind
          (shows ''__TEST_HARNESS_ASSERT_EQ_PTR '' o
            shows_paren
             ( shows ca o shows '', '' o shows (base_var_name i)))
    ) l"
\end{lstlisting}


And for each block we generate a \verb|Discover| instruction for, we must also pretty print a declaration for each of the variables we use for testing.
We do so in the following way:

\begin{lstlisting}[mathescape=true, frame=single]
  definition tests_variables :: "test_instr list $\Rightarrow$ nat $\Rightarrow$ shows" where
    "tests_variables l ind $\equiv$ foldr ($\lambda$
      (Discover _ i) $\Rightarrow$
        indent_basic ind
          (shows dflt_type o shows '' *'' o shows (base_var_name i))
      | _ $\Rightarrow$ id
      ) l"
\end{lstlisting}


Finally, we can get the list of test instructions that must be generated by using \verb|emit_global_tests|, given a list of variables it will generate a list of test instructions which we will then generate C code for.
This function is defined as follows:


\begin{lstlisting}[mathescape=true, frame=single]
definition
  emit_globals_tests ::
    "vname list $\Rightarrow$ state $\rightharpoonup$ (nat set $\times$ test_instr list)"
where "emit_globals_tests $\equiv$ $\lambda$vnames ($\sigma$,$\gamma$,$\mu$).
  fold_option ($\lambda$x (D,emit). do {
    case $\gamma$ x of
      Some vo $\Rightarrow$ do {
        let cai = x;
        case vo of
            None $\Rightarrow$ Some (D,emit)
          | Some (I v) $\Rightarrow$ Some (D,emit @ [Assert_Eq cai v])
          | Some (NullVal) $\Rightarrow$ Some (D,emit @ [Assert_Eq_Null cai] )
          | Some (A addr) $\Rightarrow$ do {
              (D,emit $\leftarrow$ dfs $\mu$ D addr cai;
              Some (D,emit@emit')
            }
      }
    | _ $\Rightarrow$ Some (D,emit)
  }
  ) vnames ({},[])"
\end{lstlisting}


\section{Tests}

\subsection{Generation of code with tests}\label{subsection:codegen_with_tests}

In section~\ref{section:exporting_c_code} we described a way of exporting C programs.
We have a second way to export C programs where we also generate C code for tests regarding the equality of final states.

We defined already the way in which every construct necessary for tests is pretty printed, now we proceed to describe how this test code is generated.

We have a similar function to prepare a program for exporting code with tests, it is defined in figure~\ref{fig:prepare_test_export} (where $\Downarrow$ stands for the new line character).
We first get the code for the program without tests by using the \verb|prepare_export| function.
We only generate test code for valid programs whose execution yields a final state, therefore we must check that by executing the program.
Then we create the list of tests for the global variables.
Later we generate the string for the variable declarations, a string for initializing the hash set and the string for the actual calls to the macros defined in C.
We also generate an instruction that checks for failed tests, we add this instruction which will check the number of tests executed and how many of those failed and passed and will proceed to print to the standard output the results.
Finally, \verb|prepare_test_export| will yield a tuple which contains the code for the program without tests, and the respective tests for it.

\begin{figure}
\begin{lstlisting}[mathescape=true]
definition prepare_test_export :: "program $\Rightarrow$ (string $\times$ string) option"
where "prepare_test_export prog $\equiv$ do {
  code $\leftarrow$ prepare_export prog;
  s $\leftarrow$ execute prog;
  let vnames = program.globals prog;
  (_,tests) $\leftarrow$ emit_globals_tests vnames s;
  let vars = tests_variables tests 1 '''';
  let instrs = tests_instructions tests 1 '''';
  let failed_check = failed_check prog;
  let init_hash = init_disc;
  let nl = ''$\Downarrow$'';
  let test_code =
    nl @ vars @ nl @ init_hash @ nl @ instrs @ nl @ failed_check @ nl @ ''}'';
  Some (code,test_code)
}"
\end{lstlisting}

\caption{Function that prepares a program for test export}
\label{fig:prepare_test_export}
\end{figure}


In order to export the code with the added tests we define a new ML function called \verb|generate_c_test_code|, its definition is on figure~\ref{fig:generate_c_test_code}.
The first parameter of the function is a \verb|(string, string) option| which corresponds to the tuple containing the string representation of the program in C code and the tests for that program, respectively.
If it receives a \verb|None| value, this means the \verb|prepare_test_export| function failed and we do not want to generate a C program for it.
The second parameter is the path to the directory where we want the program to be exported, this parameter is a relative path to the directory where the theory containing the pretty printing directives is.
The third parameter is the name of the program.
Finally, the last parameter is the theory context we are in.

If the path given is an empty string the generated C code with tests will be pretty printed to Isabelle's output view.
This function will create a new file with the name indicated in the parameters with an extra "\verb|.c|" (i.e.\ $<name>\mathtt{.c}$) added in the directory indicated by the path parameter.
The user will then be able to find the generated code in the directory indicated.
This function works by writing the C code for the program in a file and appending the tests we generate for the program at the end of the main function.

The function \verb|expect_failed_test| is very similar to the \verb|expect_failed_export| function presented in section~\ref{section:exporting_c_code} but with a different error message.
This function will then generate an error in Isabelle if code is generated for the program and tests when we expect the translation process to fail and it will do nothing when the code is not generated.

\begin{figure}
\begin{lstlisting}[mathescape=true]
  fun generate_c_test_code (SOME (code,test_code)) rel_path name thy =
    let
      val code = code |> String.implode
      val test_code = test_code |> String.implode
    in
      if rel_path="" orelse name="" then
        (writeln (code ^ " <rem last line> " ^ test_code); thy)
      else let
        val base_path = Resources.master_directory thy
        val rel_path = Path.explode rel_path
        val name_path = Path.basic name |> Path.ext "c"

        val abs_path = Path.appends [base_path, rel_path, name_path]
        val abs_path = Path.implode abs_path

        val _ = writeln ("Writing to file " ^ abs_path)

        val os = TextIO.openOut abs_path;
        val _ = TextIO.output (os, code);
        val _ = TextIO.flushOut os;
        val _ = TextIO.closeOut os;

        val _ = Isabelle_System.bash ("sed -i '$\mathtt{\$}$d ' " ^ abs_path);

        val os = TextIO.openAppend abs_path;
        val _ = TextIO.output (os, test_code);
        val _ = TextIO.flushOut os;
        val _ = TextIO.closeOut os;
      in thy end
    end

  | generate_c_test_code NONE _ _ _ =
      error "Invalid program or failed execution"

  fun expect_failed_test (SOME _) = error "Expected Failed test"
    | expect_failed_test NONE = ()
\end{lstlisting}

\caption{Generation of C code with tests}
\label{fig:generate_c_test_code}
\end{figure}


\subsection{Incorrect tests}

When developing a test suite it is also important to consider the incorrect cases.
We wrote a set of programs for which we expect code generation to fail.
By using the functions \verb|expect_failed_export| and \verb|expect_failed_test| defined in sections~\ref{section:exporting_c_code} and~\ref{subsection:codegen_with_tests}, respectively, we can write incorrect programs and when generating C code for them we can tell Isabelle to expect those processes to fail and not to raise an error.

Having incorrect programs is very useful because they serve as regression tests.
When adding new features to our semantics we can then run all the tests in our test suite.
Then, if for some reason, any of those programs gets successfully executed in our semantics and C code is generated, we will have an error in Isabelle that indicates code is being generated for a program we expect to fail.
If the changes we make when adding new features somehow change the semantics we had, then these incorrect tests will help us detect those kind of errors faster.

However, it is important to note that in order for a regression test suite to be useful it most cover as many cases as possible, which, when working with a bigger language than Chloe, requires a lot of tests written.

\section{Example programs}

Apart from the tests described in this section we also present a set of example programs in Chloe.
This are meant to show how programs are written in Chloe and how the execution and code generation work.
The list of example programs included in the source code are:

\begin{itemize}
  \item{Bubblesort: implementation of the bubblesort sorting algorithm.}
  \item{Count: implementation of a function that counts the occurrences of an element in an array.}
  \item{Cyclic linked list: implementation of a cyclic single linked list.}
  \item{Factorial: implementation of the factorial function.}
  \item{Fibonacci: implementation of a function that computes the Fibonacci number of some number.}
  \item{Linked list: implementation of a single linked list.}
  \item{Mergesort: implementation of the mergesort sorting algorithm.}
  \item{Minimum: implementation of a function that returns the minimum element of an array.}
  \item{Quicksort: implementation of the quicksort sorting algorithm.}
  \item{Selectionsort: implementation of the selection sort sorting algorithm.}
  \item{String length: implementation of a function that computes the length of a string ending in zero.}
\end{itemize}

\section{Running Tests}

\subsection{Running tests in Isabelle}

Finally, in the source code submitted with this work we have a directory which includes all the tests and example programs written for Chloe.
We need a way of running those tests in Isabelle automatically.

In the source code we have also included an Isabelle theory called ``\verb|All_Tests.thy|'' which is simply an Isabelle theory that imports every test written for Isabelle, both incorrect and correct.
When we open this file in Isabelle all the theory files corresponding to the tests and example programs will be loaded.
We will then have two cases for every test.

For regular tests and example programs, code will be generated.
For incorrect tests, no code will be generated.
In the case where an error occurs, whether it is code being generated for an invalid tests, or code not being generated for a valid tests, we will have an error and it is possible to view those in the `Theories' view of Isabelle's GUI since theory files with an error get marked in red.

\subsection{Running tests in C}

When code is successfully generated we will want to compile and run the tests in an automated way.
We have a Makefile that will compile every test in the test suite as well as a bash shell script which will run every test in the test suite.
The result of running the tests will be the number of tests passed and/or failed.
When a test is failed the output is printed in red so it is visible to the user.
Also other behaviors are caught by the script and informed to the user, such as out of memory errors, segmentation faults and programs that exited with any of the reserved exit codes~\parencite{bash-scripting}.
This script is presented in figure~\ref{fig:bash_script}.

\begin{figure}
\begin{lstlisting}
#!/bin/bash

TEST_NAMES=(bubblesort_test count_test fact_test fib_test
  mergesort_test min_test occurs_test quicksort_test selection_test
  strlen_test plus_test subst_test outer_scope_test local_scope_test
  global_scope_test global_scope2_test mod_test div_test mult_test
  less_test and_test or_test not_test eq_test new_test deref_test
  while_test returnv_test linked_list_test cyclic_linked_list_test)

for test_name in ${TEST_NAMES[@]}
do
  res=$(./${test_name});
  ret=$?

if [[ ${res} == Failed* ]];
  then
    echo -e "\e[31mFAILED: \e[39m${res}"
else
  case ${ret} in

  1) echo -e "\e[31mError\e[39m (general error)
    occurred in the execution of ${test_name}";;
  2) echo -e "\e[31mError\e[39m (misuse of shell builtins)
    occurred in the execution of ${test_name}";;
  3) echo -e "\e[31mMemory allocation error\e[39m ocurred
    in execution of ${test_name}";;
  126) echo -e "\e[31mError\e[39m (command invoked cannot execute)
    occurred in the execution of ${test_name}";;
  127) echo -e "\e[31mError\e[39m (command not found)
    occurred in the execution of ${test_name}";;
  128) echo -e "\e[31mError\e[39m (invalid argument given to exit)
    occurred in the execution of ${test_name}";;
  130) echo -e "\e[31mError\e[39m (program terminated by Ctrl+C)
    occurred in the execution of ${test_name}";;
  139) echo -e "\e[31mSegmentation fault\e[39m ocurred
    in execution of ${test_name}";;
  *) echo ${res};;
  esac
fi
done
\end{lstlisting}

\caption{Shell script for running tests}
\label{fig:bash_script}
\end{figure}
